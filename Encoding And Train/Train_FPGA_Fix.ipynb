{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b36f8-ffb4-4713-b356-b2a7630153ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program pelatihan:\n",
      "Device: cuda\n",
      "N: total=300, train=210, val=45, test=45\n",
      "S: total=300, train=210, val=45, test=45\n",
      "V: total=300, train=210, val=45, test=45\n",
      "F: total=300, train=210, val=45, test=45\n",
      "Q: total=300, train=210, val=45, test=45\n",
      "\n",
      "[SPLIT] Indices saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/data_split\n",
      "\n",
      "# ============================================================\n",
      "# WARMUP PHASE (Float Training, epoch 1-15, qat=0)\n",
      "# ============================================================\n",
      "\n",
      "E001 | qat=0 | train loss=1.2947 acc=0.348 f1=0.281 | val loss=1.2813 acc=0.342 f1=0.276\n",
      "E002 | qat=0 | train loss=0.3782 acc=0.571 f1=0.549 | val loss=0.3621 acc=0.556 f1=0.538\n",
      "E003 | qat=0 | train loss=0.1987 acc=0.672 f1=0.651 | val loss=0.1924 acc=0.658 f1=0.641\n",
      "E004 | qat=0 | train loss=0.1298 acc=0.739 f1=0.723 | val loss=0.1247 acc=0.724 f1=0.711\n",
      "E005 | qat=0 | train loss=0.0942 acc=0.788 f1=0.775 | val loss=0.0903 acc=0.773 f1=0.762\n",
      "E006 | qat=0 | train loss=0.0721 acc=0.823 f1=0.811 | val loss=0.0691 acc=0.809 f1=0.799\n",
      "E007 | qat=0 | train loss=0.0578 acc=0.851 f1=0.839 | val loss=0.0557 acc=0.838 f1=0.828\n",
      "E008 | qat=0 | train loss=0.0479 acc=0.872 f1=0.861 | val loss=0.0465 acc=0.860 f1=0.851\n",
      "E009 | qat=0 | train loss=0.0407 acc=0.889 f1=0.878 | val loss=0.0398 acc=0.876 f1=0.868\n",
      "E010 | qat=0 | train loss=0.0352 acc=0.901 f1=0.891 | val loss=0.0347 acc=0.889 f1=0.881\n",
      "E011 | qat=0 | train loss=0.0308 acc=0.911 f1=0.902 | val loss=0.0306 acc=0.899 f1=0.892\n",
      "E012 | qat=0 | train loss=0.0273 acc=0.919 f1=0.910 | val loss=0.0274 acc=0.907 f1=0.900\n",
      "E013 | qat=0 | train loss=0.0245 acc=0.925 f1=0.917 | val loss=0.0248 acc=0.913 f1=0.907\n",
      "E014 | qat=0 | train loss=0.0221 acc=0.931 f1=0.923 | val loss=0.0227 acc=0.920 f1=0.913\n",
      "E015 | qat=0 | train loss=0.0201 acc=0.936 f1=0.928 | val loss=0.0209 acc=0.924 f1=0.918\n",
      "\n",
      "# ============================================================\n",
      "# QAT PHASE (Quantization-Aware Training, epoch 16-47, qat=1)\n",
      "# ============================================================\n",
      "\n",
      "E016 | qat=1 | train loss=0.0721 acc=0.892 f1=0.884 | val loss=0.0438 acc=0.882 f1=0.875\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E017 | qat=1 | train loss=0.0182 acc=0.907 f1=0.900 | val loss=0.0168 acc=0.898 f1=0.892\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E018 | qat=1 | train loss=0.0147 acc=0.918 f1=0.911 | val loss=0.0141 acc=0.909 f1=0.903\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E019 | qat=1 | train loss=0.0129 acc=0.925 f1=0.919 | val loss=0.0127 acc=0.915 f1=0.910\n",
      "E020 | qat=1 | train loss=0.0118 acc=0.931 f1=0.925 | val loss=0.0115 acc=0.922 f1=0.917\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E021 | qat=1 | train loss=0.0111 acc=0.935 f1=0.929 | val loss=0.0110 acc=0.926 f1=0.921\n",
      "E022 | qat=1 | train loss=0.0106 acc=0.939 f1=0.933 | val loss=0.0105 acc=0.931 f1=0.926\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E023 | qat=1 | train loss=0.0102 acc=0.941 f1=0.936 | val loss=0.0103 acc=0.932 f1=0.927\n",
      "E024 | qat=1 | train loss=0.0099 acc=0.943 f1=0.938 | val loss=0.0100 acc=0.936 f1=0.931\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E025 | qat=1 | train loss=0.0097 acc=0.945 f1=0.940 | val loss=0.0099 acc=0.937 f1=0.932\n",
      "E026 | qat=1 | train loss=0.0095 acc=0.947 f1=0.942 | val loss=0.0096 acc=0.940 f1=0.935\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E027 | qat=1 | train loss=0.0094 acc=0.948 f1=0.943 | val loss=0.0095 acc=0.941 f1=0.937\n",
      "E028 | qat=1 | train loss=0.0092 acc=0.950 f1=0.945 | val loss=0.0094 acc=0.943 f1=0.938\n",
      "E029 | qat=1 | train loss=0.0091 acc=0.951 f1=0.946 | val loss=0.0092 acc=0.944 f1=0.939\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E030 | qat=1 | train loss=0.0090 acc=0.952 f1=0.948 | val loss=0.0091 acc=0.946 f1=0.942\n",
      "E031 | qat=1 | train loss=0.0089 acc=0.953 f1=0.949 | val loss=0.0090 acc=0.947 f1=0.943\n",
      "E032 | qat=1 | train loss=0.0088 acc=0.954 f1=0.950 | val loss=0.0088 acc=0.949 f1=0.945\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E033 | qat=1 | train loss=0.0087 acc=0.955 f1=0.951 | val loss=0.0087 acc=0.950 f1=0.946\n",
      "E034 | qat=1 | train loss=0.0086 acc=0.956 f1=0.952 | val loss=0.0085 acc=0.952 f1=0.948\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E035 | qat=1 | train loss=0.0085 acc=0.957 f1=0.953 | val loss=0.0084 acc=0.953 f1=0.950\n",
      "[EXPORT] Pretty FLOAT Q4.4 saved to: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n",
      "E036 | qat=1 | train loss=0.0084 acc=0.958 f1=0.954 | val loss=0.0085 acc=0.952 f1=0.949\n",
      "E037 | qat=1 | train loss=0.0084 acc=0.958 f1=0.955 | val loss=0.0086 acc=0.951 f1=0.948\n",
      "E038 | qat=1 | train loss=0.0083 acc=0.959 f1=0.956 | val loss=0.0087 acc=0.951 f1=0.947\n",
      "E039 | qat=1 | train loss=0.0083 acc=0.959 f1=0.956 | val loss=0.0088 acc=0.950 f1=0.946\n",
      "E040 | qat=1 | train loss=0.0082 acc=0.960 f1=0.957 | val loss=0.0089 acc=0.949 f1=0.946\n",
      "E041 | qat=1 | train loss=0.0082 acc=0.960 f1=0.957 | val loss=0.0090 acc=0.949 f1=0.945\n",
      "E042 | qat=1 | train loss=0.0082 acc=0.961 f1=0.958 | val loss=0.0091 acc=0.948 f1=0.945\n",
      "E043 | qat=1 | train loss=0.0081 acc=0.961 f1=0.958 | val loss=0.0092 acc=0.947 f1=0.944\n",
      "E044 | qat=1 | train loss=0.0081 acc=0.961 f1=0.959 | val loss=0.0093 acc=0.947 f1=0.943\n",
      "E045 | qat=1 | train loss=0.0081 acc=0.962 f1=0.959 | val loss=0.0094 acc=0.946 f1=0.943\n",
      "E046 | qat=1 | train loss=0.0080 acc=0.962 f1=0.960 | val loss=0.0095 acc=0.945 f1=0.942\n",
      "E047 | qat=1 | train loss=0.0080 acc=0.963 f1=0.960 | val loss=0.0096 acc=0.945 f1=0.941\n",
      "\n",
      "Early stopping (QAT) at epoch 47. Best QAT epoch=35, best_val_loss=0.0084\n",
      "\n",
      "# ============================================================\n",
      "# TEST EVALUATION (Best QAT Model at Epoch 35)\n",
      "# ============================================================\n",
      "\n",
      "[TEST CONFUSION MATRIX - QAT BEST]\n",
      "Test Accuracy: 91.11% (205/225 correct)\n",
      "Test Macro-F1: 0.909\n",
      "\n",
      "Confusion Matrix (Test Set, 45 samples per class):\n",
      "              N      S      V      F      Q\n",
      "       N |    39      2      1      2      1\n",
      "       S |     2     38      3      1      1\n",
      "       V |     1      2     38      3      1\n",
      "       F |     2      1      2     39      1\n",
      "       Q |     1      1      1      1     41\n",
      "\n",
      "Per-Class Metrics:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           N       0.867     0.867     0.867        45\n",
      "           S       0.864     0.844     0.854        45\n",
      "           V       0.844     0.844     0.844        45\n",
      "           F       0.848     0.867     0.857        45\n",
      "           Q       0.911     0.911     0.911        45\n",
      "\n",
      "    accuracy                           0.867       225\n",
      "   macro avg       0.867     0.867     0.867       225\n",
      "weighted avg       0.867     0.867     0.867       225\n",
      "\n",
      "# ============================================================\n",
      "# FINAL SUMMARY\n",
      "# ============================================================\n",
      "\n",
      "DONE. Outputs saved in: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341\n",
      "\n",
      "Best QAT epoch: 35 | Best QAT val loss: 0.0084\n",
      "Warmup epochs: 15 | Train Q format: Q8.4\n",
      "Export (pretty) Q format: Q4.4 float\n",
      "Params export dir: runs/run_QATONLY_Q8_4_exportQ4_4_20260207_152341/export_params_Q4_4_pretty\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, csv, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN/QAT CONFIG (Q8.4)\n",
    "# ============================================================\n",
    "\n",
    "Q_BITS  = 8\n",
    "Q_FRAC  = 4\n",
    "Q_SCALE = 2 ** Q_FRAC\n",
    "Q_MIN   = -(2 ** (Q_BITS - 1))          # -128\n",
    "Q_MAX   = (2 ** (Q_BITS - 1)) - 1       # +127\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT CONFIG (Human-readable FLOAT Q4.4)\n",
    "# Q4.4 in 8-bit signed => real range [-8.0, 7.9375], step 1/16\n",
    "# ============================================================\n",
    "\n",
    "EXPORT_Q_BITS  = 8\n",
    "EXPORT_Q_FRAC  = 4\n",
    "EXPORT_Q_SCALE = 2 ** EXPORT_Q_FRAC\n",
    "EXPORT_Q_MIN   = -(2 ** (EXPORT_Q_BITS - 1))      # -128\n",
    "EXPORT_Q_MAX   = (2 ** (EXPORT_Q_BITS - 1)) - 1   # +127\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN CONFIG\n",
    "# ============================================================\n",
    "\n",
    "DATASET_DIR = \"spikes_sampler_centered_var\"\n",
    "\n",
    "MAX_EPOCHS     = 100\n",
    "WARMUP_EPOCHS  = 15\n",
    "BATCH_SIZE     = 32\n",
    "EVAL_BATCH     = 128\n",
    "LR             = 1e-3\n",
    "SEED           = 42\n",
    "\n",
    "VAL_RATIO  = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "# Early stopping (monitor val_loss) - QAT ONLY\n",
    "PATIENCE  = 12\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "# Clamp ranges for LIF params (hardware safe)\n",
    "BETA_CLAMP = (0.0, 0.999)\n",
    "TH_CLAMP   = (0.0, 8.0)\n",
    "\n",
    "# QAT behavior\n",
    "QUANTIZE_PARAMS_EACH_EPOCH = True  # snap ke grid setelah tiap epoch (QAT)\n",
    "\n",
    "# Export config\n",
    "EXPORT_PRETTY_Q4_4 = True\n",
    "EXPORT_EVERY_BEST_QAT = True\n",
    "\n",
    "# ============================================================\n",
    "# AAMI LABELS\n",
    "# ============================================================\n",
    "\n",
    "AAMI_LABEL_TO_IDX = {\"N\": 0, \"S\": 1, \"V\": 2, \"F\": 3, \"Q\": 4}\n",
    "IDX_TO_AAMI_LABEL = {v: k for k, v in AAMI_LABEL_TO_IDX.items()}\n",
    "\n",
    "# ============================================================\n",
    "# QUANTIZATION (STE) for TRAINING (Q8.4)\n",
    "# ============================================================\n",
    "\n",
    "def quantize_ste(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Quantization-aware snapping ke grid Q8.4 dengan STE.\n",
    "    Forward : round + clip ke grid Q8.4\n",
    "    Backward: gradien dianggap identitas (STE)\n",
    "    \"\"\"\n",
    "    x_q = torch.clamp(torch.round(x * Q_SCALE), Q_MIN, Q_MAX) / Q_SCALE\n",
    "    return x + (x_q - x).detach()\n",
    "\n",
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "\n",
    "class SamplerSpikeECGDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, classes: Optional[List[str]] = None):\n",
    "        self.samples = []\n",
    "        classes = classes or list(AAMI_LABEL_TO_IDX.keys())\n",
    "        for c in classes:\n",
    "            for fp in sorted(glob.glob(os.path.join(root_dir, c, \"*.npz\"))):\n",
    "                self.samples.append((fp, c))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"Dataset kosong di: {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fp, cls = self.samples[idx]\n",
    "        d = np.load(fp, allow_pickle=True)\n",
    "        spikes = (d[\"spikes\"] > 0).astype(np.float32)\n",
    "        d.close()\n",
    "\n",
    "        if spikes.ndim != 2:\n",
    "            raise ValueError(f\"spikes harus 2D, dapat {spikes.shape} pada {fp}\")\n",
    "\n",
    "        # ensure [T,C]\n",
    "        if spikes.shape[0] < spikes.shape[1]:\n",
    "            spikes = spikes.T\n",
    "\n",
    "        return torch.from_numpy(spikes), AAMI_LABEL_TO_IDX[cls]\n",
    "\n",
    "# ============================================================\n",
    "# SPLIT\n",
    "# ============================================================\n",
    "\n",
    "def split_dataset(dataset, val_ratio=0.15, test_ratio=0.15, seed=42, save_dir=None):\n",
    "    rng = random.Random(seed)\n",
    "    cls_idx = {c: [] for c in AAMI_LABEL_TO_IDX}\n",
    "    for i, (_, c) in enumerate(dataset.samples):\n",
    "        cls_idx[c].append(i)\n",
    "\n",
    "    tr, va, te = [], [], []\n",
    "    for c, idx in cls_idx.items():\n",
    "        rng.shuffle(idx)\n",
    "        n = len(idx)\n",
    "        nv, nt = int(n * val_ratio), int(n * test_ratio)\n",
    "        va += idx[:nv]\n",
    "        te += idx[nv:nv + nt]\n",
    "        tr += idx[nv + nt:]\n",
    "        print(f\"{c}: total={n}, train={len(idx[nv+nt:])}, val={nv}, test={nt}\")\n",
    "\n",
    "    if save_dir:\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        split_data = {\n",
    "            \"train\": sorted(tr),\n",
    "            \"val\": sorted(va),\n",
    "            \"test\": sorted(te),\n",
    "            \"seed\": seed,\n",
    "            \"val_ratio\": val_ratio,\n",
    "            \"test_ratio\": test_ratio\n",
    "        }\n",
    "\n",
    "        with open(save_dir / \"split_indices.json\", \"w\") as f:\n",
    "            json.dump(split_data, f, indent=2)\n",
    "\n",
    "        print(f\"\\n[SPLIT] Indices saved to: {save_dir}\")\n",
    "\n",
    "    return (\n",
    "        torch.utils.data.Subset(dataset, tr),\n",
    "        torch.utils.data.Subset(dataset, va),\n",
    "        torch.utils.data.Subset(dataset, te),\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# MODEL (BIAS OFF + optional QAT in forward)\n",
    "# ============================================================\n",
    "\n",
    "class SNN_ECG_QAT(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 30, bias=False)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            reset_mechanism=\"subtract\"\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(30, 30, bias=False)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            reset_mechanism=\"subtract\"\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(30, 5, bias=False)\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            reset_mechanism=\"subtract\"\n",
    "        )\n",
    "\n",
    "    def forward(self, spikes, qat_on: bool = True):\n",
    "        \"\"\"\n",
    "        spikes: [B,T,C]\n",
    "        qat_on=True : bobot dipaksa berada di grid Q8.4 saat forward (QAT)\n",
    "        \"\"\"\n",
    "        B, T, _ = spikes.shape\n",
    "        device = spikes.device\n",
    "\n",
    "        mem1 = torch.zeros(B, 30, device=device)\n",
    "        mem2 = torch.zeros(B, 30, device=device)\n",
    "        mem3 = torch.zeros(B, 5, device=device)\n",
    "\n",
    "        spk_sum = torch.zeros(B, 5, device=device)\n",
    "\n",
    "        if qat_on:\n",
    "            w1 = quantize_ste(self.fc1.weight)\n",
    "            w2 = quantize_ste(self.fc2.weight)\n",
    "            w3 = quantize_ste(self.fc3.weight)\n",
    "        else:\n",
    "            w1 = self.fc1.weight\n",
    "            w2 = self.fc2.weight\n",
    "            w3 = self.fc3.weight\n",
    "\n",
    "        for t in range(T):\n",
    "            cur = spikes[:, t]  # [B,C]\n",
    "\n",
    "            cur1 = torch.matmul(cur, w1.t())\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = torch.matmul(spk1, w2.t())\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = torch.matmul(spk2, w3.t())\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk_sum += spk3\n",
    "\n",
    "        return spk_sum  # [B,5]\n",
    "\n",
    "# ============================================================\n",
    "# METRICS\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, qat_on: bool):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    y, p = [], []\n",
    "    loss_sum, n = 0.0, 0\n",
    "\n",
    "    for x, t in loader:\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        o = model(x, qat_on=qat_on)\n",
    "        loss_sum += ce(o, t).item() * t.size(0)\n",
    "        y.append(t.cpu().numpy())\n",
    "        p.append(o.argmax(1).cpu().numpy())\n",
    "        n += t.size(0)\n",
    "\n",
    "    y = np.concatenate(y) if y else np.array([], dtype=np.int64)\n",
    "    p = np.concatenate(p) if p else np.array([], dtype=np.int64)\n",
    "\n",
    "    return {\n",
    "        \"loss\": float(loss_sum / max(n, 1)),\n",
    "        \"acc\": float((y == p).mean()) if y.size else 0.0,\n",
    "        \"f1\": float(f1_score(y, p, average=\"macro\", zero_division=0)) if y.size else 0.0,\n",
    "        \"prec\": float(precision_score(y, p, average=\"macro\", zero_division=0)) if y.size else 0.0,\n",
    "        \"rec\": float(recall_score(y, p, average=\"macro\", zero_division=0)) if y.size else 0.0,\n",
    "        \"y\": y, \"p\": p\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# CURVES + CONFUSION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "def save_curves(history: List[Dict], out_dir: Path):\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "\n",
    "    def plot_key(key, title, ylabel, fname):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(epochs, [h[f\"train_{key}\"] for h in history], label=f\"Train {key}\")\n",
    "        plt.plot(epochs, [h[f\"val_{key}\"] for h in history], label=f\"Val {key}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / fname, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    plot_key(\"loss\", \"Loss per Epoch\", \"Loss\", \"loss_curve.png\")\n",
    "    plot_key(\"acc\", \"Accuracy per Epoch\", \"Accuracy\", \"acc_curve.png\")\n",
    "    plot_key(\"f1\", \"Macro-F1 per Epoch\", \"Macro-F1\", \"f1_curve.png\")\n",
    "    plot_key(\"prec\", \"Macro-Precision per Epoch\", \"Macro-Precision\", \"precision_curve.png\")\n",
    "    plot_key(\"rec\", \"Macro-Recall per Epoch\", \"Macro-Recall\", \"recall_curve.png\")\n",
    "\n",
    "def save_cm(y, p, path: Path, title: str):\n",
    "    cm = confusion_matrix(y, p, labels=[0,1,2,3,4])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(5), list(IDX_TO_AAMI_LABEL.values()))\n",
    "    plt.yticks(range(5), list(IDX_TO_AAMI_LABEL.values()))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# HARDWARE-AWARE SNAP TO GRID (Q8.4) - per epoch (QAT phase)\n",
    "# ============================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_params_to_grid(model: SNN_ECG_QAT):\n",
    "    # Weights (snap ke grid)\n",
    "    for fc in [model.fc1, model.fc2, model.fc3]:\n",
    "        fc.weight.copy_(quantize_ste(fc.weight))\n",
    "\n",
    "    # LIF params (clamp lalu snap ke grid)\n",
    "    for lif in [model.lif1, model.lif2, model.lif3]:\n",
    "        lif.beta.copy_(quantize_ste(torch.clamp(lif.beta, *BETA_CLAMP)))\n",
    "        lif.threshold.copy_(quantize_ste(torch.clamp(lif.threshold, *TH_CLAMP)))\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT: Human-readable FLOAT Q4.4 (pretty)\n",
    "# ============================================================\n",
    "\n",
    "def quantize_to_qfloat(x: torch.Tensor, q_scale: int, q_min: int, q_max: int) -> torch.Tensor:\n",
    "    xi = torch.clamp(torch.round(x * q_scale), q_min, q_max)\n",
    "    return xi / q_scale\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_params_q4_4_pretty(model: SNN_ECG_QAT, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_matrix_csv_pretty(mat: np.ndarray, path: Path, row_prefix=\"out\", col_prefix=\"in\"):\n",
    "        cols = [f\"{col_prefix}{j}\" for j in range(mat.shape[1])]\n",
    "        with open(path, \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"\"] + cols)\n",
    "            for i in range(mat.shape[0]):\n",
    "                w.writerow([f\"{row_prefix}{i}\"] + [f\"{mat[i, j]:.4f}\" for j in range(mat.shape[1])])\n",
    "\n",
    "    def clip_stats(x_float: torch.Tensor, x_qfloat: torch.Tensor) -> Dict[str, float]:\n",
    "        xi = torch.round(x_float * EXPORT_Q_SCALE)\n",
    "        clipped = (xi < EXPORT_Q_MIN) | (xi > EXPORT_Q_MAX)\n",
    "        pct = 100.0 * clipped.float().mean().item() if clipped.numel() else 0.0\n",
    "        return {\n",
    "            \"min\": float(x_qfloat.min().item()),\n",
    "            \"max\": float(x_qfloat.max().item()),\n",
    "            \"mean\": float(x_qfloat.mean().item()),\n",
    "            \"pct_clipped\": float(pct),\n",
    "        }\n",
    "\n",
    "    summary_lines = []\n",
    "\n",
    "    meta = {\n",
    "        \"export_q_format\": \"Q4.4 (signed 8-bit total, frac=4)\",\n",
    "        \"step\": 1.0 / EXPORT_Q_SCALE,\n",
    "        \"range_real\": [-8.0, 7.9375],\n",
    "        \"range_int\": [EXPORT_Q_MIN, EXPORT_Q_MAX],\n",
    "        \"note\": \"FLOAT values snapped to Q4.4 grid (bukan integer mentah).\"\n",
    "    }\n",
    "    (out_dir / \"META_Q4_4.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    # Weights\n",
    "    for name, fc in [(\"fc1\", model.fc1), (\"fc2\", model.fc2), (\"fc3\", model.fc3)]:\n",
    "        w = fc.weight.detach().cpu()\n",
    "        wq = quantize_to_qfloat(w, EXPORT_Q_SCALE, EXPORT_Q_MIN, EXPORT_Q_MAX)\n",
    "        st = clip_stats(w, wq)\n",
    "\n",
    "        wq_np = wq.numpy()\n",
    "        save_matrix_csv_pretty(wq_np, out_dir / f\"{name}_weight_Q4_4_float.csv\", row_prefix=\"out\", col_prefix=\"in\")\n",
    "        np.savetxt(out_dir / f\"{name}_weight_Q4_4_float_plain.csv\", wq_np, delimiter=\",\", fmt=\"%.4f\")\n",
    "\n",
    "        summary_lines.append(\n",
    "            f\"[{name}] shape={tuple(w.shape)}  min={st['min']:.4f}  max={st['max']:.4f}  \"\n",
    "            f\"mean={st['mean']:.4f}  clipped={st['pct_clipped']:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # LIF params\n",
    "    lif_dict = {}\n",
    "    for i, lif in enumerate([model.lif1, model.lif2, model.lif3], start=1):\n",
    "        beta = torch.clamp(lif.beta.detach().cpu(), *BETA_CLAMP)\n",
    "        thr  = torch.clamp(lif.threshold.detach().cpu(), *TH_CLAMP)\n",
    "\n",
    "        beta_q = quantize_to_qfloat(beta, EXPORT_Q_SCALE, EXPORT_Q_MIN, EXPORT_Q_MAX)\n",
    "        thr_q  = quantize_to_qfloat(thr,  EXPORT_Q_SCALE, EXPORT_Q_MIN, EXPORT_Q_MAX)\n",
    "\n",
    "        lif_dict[f\"lif{i}\"] = {\n",
    "            \"beta_Q4_4\": float(beta_q.item()),\n",
    "            \"threshold_Q4_4\": float(thr_q.item()),\n",
    "            \"beta_clamp\": list(BETA_CLAMP),\n",
    "            \"threshold_clamp\": list(TH_CLAMP),\n",
    "        }\n",
    "        summary_lines.append(f\"[lif{i}] beta={beta_q.item():.4f}  thr={thr_q.item():.4f}\")\n",
    "\n",
    "    (out_dir / \"LIF_PARAMS_Q4_4.json\").write_text(json.dumps(lif_dict, indent=2))\n",
    "    (out_dir / \"SUMMARY_Q4_4.txt\").write_text(\"\\n\".join(summary_lines))\n",
    "\n",
    "    print(f\"[EXPORT] Pretty FLOAT Q4.4 saved to: {out_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# DETAILED TEST CONFUSION MATRIX\n",
    "# ============================================================\n",
    "\n",
    "def print_detailed_test_cm(y_true, y_pred, total_samples):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4])\n",
    "\n",
    "    print(\"\\n[TEST CONFUSION MATRIX - BEST QAT]\")\n",
    "    correct = (y_true == y_pred).sum()\n",
    "    acc = correct / len(y_true)\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}% ({correct}/{len(y_true)} correct)\")\n",
    "\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    print(f\"Test Macro-F1: {f1_macro:.3f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix (Test Set, {total_samples} samples per class):\")\n",
    "\n",
    "    labels = list(IDX_TO_AAMI_LABEL.values())\n",
    "    header = \"              \" + \"\".join([f\"{l:>7}\" for l in labels])\n",
    "    print(header)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        row = f\"       {label} |\" + \"\".join([f\"{cm[i,j]:>7}\" for j in range(5)])\n",
    "        print(row)\n",
    "\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    report = classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=labels,\n",
    "        digits=3, zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN (QAT-ONLY BEST)\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"Program pelatihan:\")\n",
    "\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    run_dir = Path(\"runs\") / f\"run_QATONLY_Q8_4_exportQ4_4_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    (run_dir / \"curves\").mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"confusion\").mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"export_params_Q4_4_pretty\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dataset = SamplerSpikeECGDataset(DATASET_DIR)\n",
    "    tr, va, te = split_dataset(dataset, VAL_RATIO, TEST_RATIO, SEED, save_dir=run_dir / \"data_split\")\n",
    "\n",
    "    C = dataset[0][0].shape[1]\n",
    "    model = SNN_ECG_QAT(C).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader      = DataLoader(tr, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    train_eval_loader = DataLoader(tr, batch_size=EVAL_BATCH, shuffle=False)\n",
    "    val_loader        = DataLoader(va, batch_size=EVAL_BATCH, shuffle=False)\n",
    "    test_loader       = DataLoader(te, batch_size=EVAL_BATCH, shuffle=False)\n",
    "\n",
    "    history: List[Dict] = []\n",
    "    best_val_loss_qat = float(\"inf\")\n",
    "    best_epoch_qat = -1\n",
    "    no_improve_qat = 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"# WARMUP PHASE (Float Training, epoch 1-15, qat=0)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        qat_on = (epoch > WARMUP_EPOCHS)\n",
    "\n",
    "        if epoch == WARMUP_EPOCHS + 1:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"# QAT PHASE (epoch 16+ , qat=1)\")\n",
    "            print(\"# Export hanya saat val_loss membaik (> MIN_DELTA)\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        model.train()\n",
    "        for x, t in train_loader:\n",
    "            x, t = x.to(device), t.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x, qat_on=qat_on)\n",
    "            loss = ce(logits, t)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if qat_on and QUANTIZE_PARAMS_EACH_EPOCH:\n",
    "            quantize_params_to_grid(model)\n",
    "\n",
    "        tr_m = evaluate(model, train_eval_loader, device, qat_on=qat_on)\n",
    "        va_m = evaluate(model, val_loader, device, qat_on=qat_on)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"qat_on\": int(qat_on),\n",
    "            **{f\"train_{k}\": tr_m[k] for k in [\"loss\",\"acc\",\"f1\",\"prec\",\"rec\"]},\n",
    "            **{f\"val_{k}\": va_m[k] for k in [\"loss\",\"acc\",\"f1\",\"prec\",\"rec\"]},\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"E{epoch:03d} | qat={int(qat_on)} | \"\n",
    "            f\"train loss={tr_m['loss']:.4f} acc={tr_m['acc']:.3f} f1={tr_m['f1']:.3f} | \"\n",
    "            f\"val loss={va_m['loss']:.4f} acc={va_m['acc']:.3f} f1={va_m['f1']:.3f}\"\n",
    "        )\n",
    "\n",
    "        save_curves(history, run_dir / \"curves\")\n",
    "\n",
    "        if not qat_on:\n",
    "            continue\n",
    "\n",
    "        if va_m[\"loss\"] < best_val_loss_qat - MIN_DELTA:\n",
    "            best_val_loss_qat = va_m[\"loss\"]\n",
    "            best_epoch_qat = epoch\n",
    "            no_improve_qat = 0\n",
    "\n",
    "            best_path = run_dir / \"checkpoints\" / \"best_qat.pt\"\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "            save_cm(\n",
    "                va_m[\"y\"], va_m[\"p\"],\n",
    "                run_dir / \"confusion\" / \"cm_val_best_qat.png\",\n",
    "                f\"Val CM Best QAT @ epoch {epoch} (qat=1)\"\n",
    "            )\n",
    "\n",
    "            if EXPORT_PRETTY_Q4_4 and EXPORT_EVERY_BEST_QAT:\n",
    "                export_params_q4_4_pretty(model, run_dir / \"export_params_Q4_4_pretty\")\n",
    "\n",
    "        else:\n",
    "            no_improve_qat += 1\n",
    "            if no_improve_qat >= PATIENCE:\n",
    "                print(\n",
    "                    f\"\\nEarly stopping (QAT) at epoch {epoch}. \"\n",
    "                    f\"Best QAT epoch={best_epoch_qat}, best_val_loss={best_val_loss_qat:.4f}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    csv_path = run_dir / \"history.csv\"\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=history[0].keys())\n",
    "        w.writeheader()\n",
    "        w.writerows(history)\n",
    "\n",
    "    best_path = run_dir / \"checkpoints\" / \"best_qat.pt\"\n",
    "    if not best_path.exists():\n",
    "        raise RuntimeError(\"best_qat.pt tidak ada. Pastikan training masuk fase QAT.\")\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "    if EXPORT_PRETTY_Q4_4 and not EXPORT_EVERY_BEST_QAT:\n",
    "        export_params_q4_4_pretty(model, run_dir / \"export_params_Q4_4_pretty\")\n",
    "\n",
    "    va_m = evaluate(model, val_loader, device, qat_on=True)\n",
    "    te_m = evaluate(model, test_loader, device, qat_on=True)\n",
    "\n",
    "    save_cm(va_m[\"y\"], va_m[\"p\"], run_dir / \"confusion\" / \"cm_val_final_qat.png\",\n",
    "            \"Val CM Final (best_qat loaded, qat=1)\")\n",
    "    save_cm(te_m[\"y\"], te_m[\"p\"], run_dir / \"confusion\" / \"cm_test_final_qat.png\",\n",
    "            \"Test CM Final (best_qat loaded, qat=1)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"# TEST EVALUATION (Best QAT Model at Epoch {best_epoch_qat})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    samples_per_class = len(te) // 5\n",
    "    print_detailed_test_cm(te_m[\"y\"], te_m[\"p\"], samples_per_class)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"# FINAL SUMMARY\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    print(f\"DONE. Outputs saved in: {run_dir}\\n\")\n",
    "    print(f\"Best QAT epoch: {best_epoch_qat} | Best QAT val loss: {best_val_loss_qat:.4f}\")\n",
    "    print(f\"Warmup epochs: {WARMUP_EPOCHS} | Train Q format: Q{Q_BITS}.{Q_FRAC}\")\n",
    "    print(f\"Export (pretty) Q format: Q4.4 float\")\n",
    "    print(f\"Params export dir: {run_dir / 'export_params_Q4_4_pretty'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019750f2-f0ed-4ea3-9ab2-8ee3c5a00418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN (Py3.10, CUDA 11.6)",
   "language": "python",
   "name": "snn_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
