{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce507d4-f668-4826-8b5d-09eb2d243497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda\n",
      "Class N: total=300, train=210, val=45, test=45\n",
      "Class S: total=300, train=210, val=45, test=45\n",
      "Class V: total=300, train=210, val=45, test=45\n",
      "Class F: total=300, train=210, val=45, test=45\n",
      "Class Q: total=300, train=210, val=45, test=45\n",
      "\n",
      "Total samples: 1500\n",
      "Train: 1050, Val: 225, Test: 225\n",
      "\n",
      "Sample shape: T=234, C=5 -> input_dim=5\n",
      "Epoch 001: train_loss=3.0226, train_acc=0.294, val_loss=1.4625, val_acc=0.360\n",
      "Epoch 002: train_loss=1.4716, train_acc=0.352, val_loss=1.4428, val_acc=0.369\n",
      "Epoch 003: train_loss=1.4418, train_acc=0.386, val_loss=1.4484, val_acc=0.396\n",
      "Epoch 004: train_loss=1.3305, train_acc=0.451, val_loss=1.0939, val_acc=0.547\n",
      "Epoch 005: train_loss=1.0300, train_acc=0.573, val_loss=0.8814, val_acc=0.587\n",
      "Epoch 006: train_loss=0.6530, train_acc=0.700, val_loss=0.6014, val_acc=0.756\n",
      "Epoch 007: train_loss=0.4913, train_acc=0.796, val_loss=0.5680, val_acc=0.747\n",
      "Epoch 008: train_loss=0.3809, train_acc=0.830, val_loss=0.3225, val_acc=0.938\n",
      "Epoch 009: train_loss=0.2132, train_acc=0.947, val_loss=0.2195, val_acc=0.969\n",
      "Epoch 010: train_loss=0.1442, train_acc=0.969, val_loss=0.1734, val_acc=0.956\n",
      "Epoch 011: train_loss=0.1046, train_acc=0.983, val_loss=0.1519, val_acc=0.978\n",
      "Epoch 012: train_loss=0.0910, train_acc=0.984, val_loss=0.1295, val_acc=0.991\n",
      "Epoch 015: train_loss=0.0766, train_acc=0.978, val_loss=0.0955, val_acc=0.996\n",
      "Epoch 016: train_loss=0.0674, train_acc=0.988, val_loss=0.1010, val_acc=0.996\n",
      "Epoch 017: train_loss=0.0495, train_acc=0.992, val_loss=0.0931, val_acc=0.996\n",
      "Epoch 018: train_loss=0.0540, train_acc=0.989, val_loss=0.1045, val_acc=0.996\n",
      "Epoch 019: train_loss=0.0440, train_acc=0.991, val_loss=0.0956, val_acc=0.996\n",
      "Epoch 020: train_loss=0.0440, train_acc=0.990, val_loss=0.1139, val_acc=0.987\n",
      "Epoch 021: train_loss=0.0493, train_acc=0.988, val_loss=0.1186, val_acc=0.982\n",
      "Epoch 022: train_loss=0.0427, train_acc=0.993, val_loss=0.0884, val_acc=0.991\n",
      "Epoch 023: train_loss=0.0386, train_acc=0.991, val_loss=0.0808, val_acc=0.996\n",
      "Epoch 024: train_loss=0.0440, train_acc=0.993, val_loss=0.0854, val_acc=0.987\n",
      "Epoch 025: train_loss=0.0331, train_acc=0.992, val_loss=0.0752, val_acc=0.991\n",
      "Epoch 026: train_loss=0.0407, train_acc=0.991, val_loss=0.1608, val_acc=0.978\n",
      "Epoch 027: train_loss=0.0379, train_acc=0.990, val_loss=0.0832, val_acc=0.996\n",
      "Epoch 028: train_loss=0.0462, train_acc=0.988, val_loss=0.1129, val_acc=0.987\n",
      "Epoch 029: train_loss=0.0412, train_acc=0.986, val_loss=0.0719, val_acc=0.991\n",
      "Epoch 030: train_loss=0.0514, train_acc=0.984, val_loss=0.1371, val_acc=0.964\n",
      "Epoch 031: train_loss=0.0561, train_acc=0.982, val_loss=0.1548, val_acc=0.956\n",
      "Epoch 032: train_loss=0.0558, train_acc=0.980, val_loss=0.0945, val_acc=0.982\n",
      "Epoch 033: train_loss=0.0393, train_acc=0.990, val_loss=0.0593, val_acc=0.991\n",
      "Epoch 034: train_loss=0.0219, train_acc=0.995, val_loss=0.0653, val_acc=0.991\n",
      "Epoch 035: train_loss=0.0173, train_acc=0.997, val_loss=0.0591, val_acc=0.991\n",
      "Epoch 036: train_loss=0.0264, train_acc=0.994, val_loss=0.0645, val_acc=0.991\n",
      "Epoch 037: train_loss=0.0223, train_acc=0.991, val_loss=0.0573, val_acc=0.991\n",
      "Epoch 038: train_loss=0.0206, train_acc=0.993, val_loss=0.0677, val_acc=0.987\n",
      "Epoch 039: train_loss=0.0157, train_acc=0.997, val_loss=0.0515, val_acc=0.987\n",
      "Epoch 040: train_loss=0.0167, train_acc=0.997, val_loss=0.0591, val_acc=0.991\n",
      "Epoch 041: train_loss=0.0172, train_acc=0.996, val_loss=0.0410, val_acc=0.991\n",
      "Epoch 042: train_loss=0.0183, train_acc=0.996, val_loss=0.0479, val_acc=0.987\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training SNN untuk ECG dengan snnTorch (3-way split: Train / Val / Test)\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Pipeline:\n",
    "1. Baca dataset spike encoding dari folder:\n",
    "       dataset_ecg_encoded/<class>/beat_xxxx.npz\n",
    "\n",
    "2. Dataset berisi channel:\n",
    "   - dm_up        : [T]\n",
    "   - dm_down      : [T]\n",
    "   - lc_up        : [T]   (level crossing LC+ 2-channel)\n",
    "   - lc_down      : [T]   (LC-)\n",
    "   - energy_spikes: [T]\n",
    "\n",
    "   Disusun menjadi spike tensor [T, C]:\n",
    "   - channel 0 : dm_up\n",
    "   - channel 1 : dm_down\n",
    "   - channel 2 : lc_up\n",
    "   - channel 3 : lc_down\n",
    "   - channel 4 : energy_spikes\n",
    "\n",
    "3. Model SNN (snnTorch):\n",
    "   Input C  → FC(30) + LIF (trainable leak & threshold)\n",
    "            → FC(30) + LIF\n",
    "            → FC(5)  + LIF\n",
    "\n",
    "   - Reset               : \"subtract\"\n",
    "   - Surrogate gradient  : atan\n",
    "   - Output              : spike count per kelas\n",
    "   - Klasifikasi         : argmax(spike_count)\n",
    "\n",
    "4. Dataset split (stratified per kelas):\n",
    "   - Train: 70%\n",
    "   - Val  : 15%\n",
    "   - Test : 15%\n",
    "\n",
    "5. Training:\n",
    "   - Loss        : CrossEntropy(spike_counts, label)\n",
    "   - Optimizer   : Adam\n",
    "   - Early stopping berdasarkan validation loss\n",
    "\n",
    "6. Output file:\n",
    "   - snn_ecg_snntorch_30_30_best.pt           (model terbaik)\n",
    "   - training_curves_snntorch.png             (loss + accuracy)\n",
    "   - confusion_matrix_val_snntorch.png        (confusion matrix validation)\n",
    "   - confusion_matrix_test_snntorch.png       (confusion matrix test)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Label mapping AAMI\n",
    "# ============================================================\n",
    "\n",
    "AAMI_LABEL_TO_IDX: Dict[str, int] = {\n",
    "    \"N\": 0,\n",
    "    \"S\": 1,\n",
    "    \"V\": 2,\n",
    "    \"F\": 3,\n",
    "    \"Q\": 4,\n",
    "}\n",
    "IDX_TO_AAMI_LABEL: Dict[int, str] = {v: k for k, v in AAMI_LABEL_TO_IDX.items()}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Dataset: membaca spike encoding dari .npz\n",
    "# ============================================================\n",
    "\n",
    "class SpikeECGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset untuk membaca spike encoding ECG dari folder.\n",
    "\n",
    "    Struktur folder:\n",
    "      root_dir/\n",
    "          N/*.npz\n",
    "          S/*.npz\n",
    "          V/*.npz\n",
    "          F/*.npz\n",
    "          Q/*.npz\n",
    "\n",
    "    Setiap file .npz minimal punya:\n",
    "      - dm_up        : [T]\n",
    "      - dm_down      : [T]\n",
    "      - lc_up        : [T]\n",
    "      - lc_down      : [T]\n",
    "      - energy_spikes: [T]\n",
    "\n",
    "    Output __getitem__:\n",
    "      - spikes: Tensor [T, C]  (T = time steps, C = channels)\n",
    "      - label : int (0..4) sesuai AAMI_LABEL_TO_IDX\n",
    "\n",
    "    Catatan:\n",
    "      - Properti self.samples berisi list (filepath, cls_str) → dipakai\n",
    "        untuk stratified split di fungsi split_dataset_3way.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, classes: List[str] = None):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        if classes is None:\n",
    "            classes = [\"N\", \"S\", \"V\", \"F\", \"Q\"]\n",
    "        # pastikan uppercase\n",
    "        self.classes = [c.upper() for c in classes]\n",
    "\n",
    "        # List (filepath, label_str)\n",
    "        self.samples: List[Tuple[str, str]] = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            files = sorted(glob.glob(os.path.join(cls_dir, \"*.npz\")))\n",
    "            for fp in files:\n",
    "                self.samples.append((fp, cls))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"Tidak ada file .npz di {root_dir}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fp, cls = self.samples[idx]\n",
    "        data = np.load(fp)\n",
    "\n",
    "        # Ambil channel spike encoding (semua [T])\n",
    "        dm_up = data[\"dm_up\"].astype(np.float32)          # [T]\n",
    "        dm_down = data[\"dm_down\"].astype(np.float32)      # [T]\n",
    "        lc_up = data[\"lc_up\"].astype(np.float32)          # [T]\n",
    "        lc_down = data[\"lc_down\"].astype(np.float32)      # [T]\n",
    "        energy_spikes = data[\"energy_spikes\"].astype(np.float32)  # [T]\n",
    "\n",
    "        # Bentuk [C, T] dengan urutan:\n",
    "        #   0 : dm_up\n",
    "        #   1 : dm_down\n",
    "        #   2 : lc_up\n",
    "        #   3 : lc_down\n",
    "        #   4 : energy_spikes\n",
    "        dm_up_ch = dm_up[None, :]          # [1, T]\n",
    "        dm_down_ch = dm_down[None, :]      # [1, T]\n",
    "        lc_up_ch = lc_up[None, :]          # [1, T]\n",
    "        lc_down_ch = lc_down[None, :]      # [1, T]\n",
    "        energy_ch = energy_spikes[None, :] # [1, T]\n",
    "\n",
    "        spikes_np = np.concatenate(\n",
    "            [dm_up_ch, dm_down_ch, lc_up_ch, lc_down_ch, energy_ch],\n",
    "            axis=0\n",
    "        )  # [C, T]\n",
    "\n",
    "        # Convert ke tensor [T, C] (time-major untuk SNN)\n",
    "        spikes = torch.from_numpy(spikes_np.T)  # [T, C]\n",
    "\n",
    "        # Label ke index\n",
    "        y = AAMI_LABEL_TO_IDX[cls]\n",
    "        return spikes, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Stratified 3-way split: Train / Val / Test\n",
    "# ============================================================\n",
    "\n",
    "def split_dataset_3way(\n",
    "    dataset: SpikeECGDataset,\n",
    "    val_ratio: float = 0.15,\n",
    "    test_ratio: float = 0.15,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Split dataset menjadi train / val / test dengan stratifikasi per kelas.\n",
    "\n",
    "    - val_ratio dan test_ratio digunakan per kelas\n",
    "    - Train ratio = 1 - val_ratio - test_ratio\n",
    "    \"\"\"\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    # Kumpulkan index berdasarkan kelas string (N, S, V, F, Q)\n",
    "    class_to_indices: Dict[str, List[int]] = {c: [] for c in AAMI_LABEL_TO_IDX.keys()}\n",
    "\n",
    "    for idx in range(len(dataset)):\n",
    "        _, cls = dataset.samples[idx]  # (filepath, cls_str)\n",
    "        class_to_indices[cls].append(idx)\n",
    "\n",
    "    train_idx: List[int] = []\n",
    "    val_idx: List[int] = []\n",
    "    test_idx: List[int] = []\n",
    "\n",
    "    for cls, indices in class_to_indices.items():\n",
    "        if not indices:\n",
    "            continue\n",
    "\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        n = len(indices)\n",
    "        n_val = int(n * val_ratio)\n",
    "        n_test = int(n * test_ratio)\n",
    "\n",
    "        cls_val = indices[:n_val]\n",
    "        cls_test = indices[n_val:n_val + n_test]\n",
    "        cls_train = indices[n_val + n_test:]\n",
    "\n",
    "        train_idx.extend(cls_train)\n",
    "        val_idx.extend(cls_val)\n",
    "        test_idx.extend(cls_test)\n",
    "\n",
    "        print(\n",
    "            f\"Class {cls}: total={n}, \"\n",
    "            f\"train={len(cls_train)}, val={len(cls_val)}, test={len(cls_test)}\"\n",
    "        )\n",
    "\n",
    "    # Buat subset\n",
    "    train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "    test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. SNN dengan snnTorch (LIF + atan surrogate)\n",
    "# ============================================================\n",
    "\n",
    "class SNN_Snntorch_ECG(nn.Module):\n",
    "    \"\"\"\n",
    "    Arsitektur SNN:\n",
    "\n",
    "    Input C  → FC(30) + Leaky LIF\n",
    "             → FC(30) + Leaky LIF\n",
    "             → FC(5)  + Leaky LIF\n",
    "\n",
    "    - Neuron: snn.Leaky dengan:\n",
    "        * beta           : leak\n",
    "        * threshold      : ambang\n",
    "        * learn_beta     : True\n",
    "        * learn_threshold: True\n",
    "        * reset_mechanism: \"subtract\"\n",
    "        * spike_grad     : surrogate.atan()\n",
    "    - Output: spike_count per neuron output (kelas)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden1: int = 30, hidden2: int = 30, n_classes: int = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1: FC -> LIF\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1, bias=True)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=0.9,\n",
    "            threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),   # surrogate gradient arctan\n",
    "            learn_beta=True,\n",
    "            learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "        # Layer 2: FC -> LIF\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2, bias=True)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=0.9,\n",
    "            threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            learn_beta=True,\n",
    "            learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "        # Layer output: FC -> LIF\n",
    "        self.fc3 = nn.Linear(hidden2, n_classes, bias=True)\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=0.9,\n",
    "            threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            learn_beta=True,\n",
    "            learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "    def forward(self, spikes: torch.Tensor):\n",
    "        \"\"\"\n",
    "        spikes: [B, T, C]  (batch, time, channel)\n",
    "        return:\n",
    "          spike_counts: [B, n_classes]\n",
    "        \"\"\"\n",
    "        B, T, C = spikes.shape\n",
    "        device = spikes.device\n",
    "\n",
    "        # snnTorch biasanya pakai [T, B, ...], jadi kita permute\n",
    "        spikes_t = spikes.permute(1, 0, 2)  # [T, B, C]\n",
    "\n",
    "        # Inisialisasi membrane potential ke nol di awal sequence\n",
    "        mem1 = torch.zeros(B, self.fc1.out_features, device=device)\n",
    "        mem2 = torch.zeros(B, self.fc2.out_features, device=device)\n",
    "        mem3 = torch.zeros(B, self.fc3.out_features, device=device)\n",
    "\n",
    "        # Akumulasi spike output untuk klasifikasi\n",
    "        spk_sum = torch.zeros(B, self.fc3.out_features, device=device)\n",
    "\n",
    "        for t in range(T):\n",
    "            cur = spikes_t[t]          # [B, C]\n",
    "\n",
    "            # Layer 1\n",
    "            cur = self.fc1(cur)\n",
    "            spk1, mem1 = self.lif1(cur, mem1)\n",
    "\n",
    "            # Layer 2\n",
    "            cur = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur, mem2)\n",
    "\n",
    "            # Output layer\n",
    "            cur = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur, mem3)\n",
    "\n",
    "            # Akumulasi spike di output neuron\n",
    "            spk_sum += spk3\n",
    "\n",
    "        # spike_sum sebagai \"logits\" untuk CrossEntropy\n",
    "        return spk_sum\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Training & Evaluation helpers\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for spikes, labels in loader:\n",
    "        spikes = spikes.to(device)  # [B, T, C]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        spike_counts = model(spikes)   # [B, 5]\n",
    "        logits = spike_counts          # gunakan spike count sebagai logits\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def eval_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spikes, labels in loader:\n",
    "            spikes = spikes.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            spike_counts = model(spikes)\n",
    "            logits = spike_counts\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def get_all_predictions(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for spikes, labels in loader:\n",
    "            spikes = spikes.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            spike_counts = model(spikes)\n",
    "            logits = spike_counts\n",
    "            preds = logits.argmax(dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return all_labels, all_preds\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    filename: str,\n",
    "    title: str,\n",
    "):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick_marks = np.arange(5)\n",
    "    class_names = [IDX_TO_AAMI_LABEL[i] for i in range(5)]\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Anotasi angka di tiap cell\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                str(cm[i, j]),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Confusion matrix saved as '{filename}'\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Main: Training loop + plot + val/test confusion matrix\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ---------------- Konfigurasi dasar ----------------\n",
    "    dataset_dir = \"dataset_ecg_encoded\"   # folder hasil encoding spike\n",
    "    batch_size = 32\n",
    "    max_epochs = 100\n",
    "    lr = 1e-3\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    patience = 10               # early stopping patience\n",
    "    num_workers = 0\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device = {device}\")\n",
    "\n",
    "    # 1) Bangun dataset dari hasil generate + encoding tadi\n",
    "    full_dataset = SpikeECGDataset(dataset_dir, classes=[\"N\", \"S\", \"V\", \"F\", \"Q\"])\n",
    "\n",
    "    # Stratified 3-way split\n",
    "    train_set, val_set, test_set = split_dataset_3way(\n",
    "        full_dataset,\n",
    "        val_ratio=val_ratio,\n",
    "        test_ratio=test_ratio,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTotal samples: {len(full_dataset)}\")\n",
    "    print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\\n\")\n",
    "\n",
    "    # Cek shape contoh untuk menentukan input_dim\n",
    "    spikes0, _ = full_dataset[0]   # [T, C]\n",
    "    T0, C0 = spikes0.shape\n",
    "    input_dim = C0\n",
    "    print(f\"Sample shape: T={T0}, C={C0} -> input_dim={input_dim}\")\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # 2) Bangun SNN (snnTorch)\n",
    "    model = SNN_Snntorch_ECG(input_dim=input_dim, hidden1=30, hidden2=30, n_classes=5)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # History untuk plot\n",
    "    train_loss_hist: List[float] = []\n",
    "    val_loss_hist: List[float] = []\n",
    "    train_acc_hist: List[float] = []\n",
    "    val_acc_hist: List[float] = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # 3) Training loop dengan early stopping (berdasarkan val_loss)\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc = eval_one_epoch(model, val_loader, device)\n",
    "\n",
    "        train_loss_hist.append(train_loss)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        train_acc_hist.append(train_acc)\n",
    "        val_acc_hist.append(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}: \"\n",
    "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, \"\n",
    "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping monitor val_loss\n",
    "        if val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "    # 4) Restore best model & save\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(best_state, \"snn_ecg_snntorch_30_30_best.pt\")\n",
    "        print(\"Best model saved to 'snn_ecg_snntorch_30_30_best.pt'\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5) Plot training curves (loss & accuracy)\n",
    "    # --------------------------------------------------------\n",
    "    epochs_range = range(1, len(train_loss_hist) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Loss curve\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs_range, train_loss_hist, label=\"Train loss\")\n",
    "    plt.plot(epochs_range, val_loss_hist, label=\"Val loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss (snnTorch)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy curve\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs_range, train_acc_hist, label=\"Train acc\")\n",
    "    plt.plot(epochs_range, val_acc_hist, label=\"Val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training & Validation Accuracy (snnTorch)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves_snntorch.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    print(\"Training curves saved as 'training_curves_snntorch.png'\")\n",
    "    plt.close()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6) Confusion matrix (validation set)\n",
    "    # --------------------------------------------------------\n",
    "    y_val_true, y_val_pred = get_all_predictions(model, val_loader, device)\n",
    "\n",
    "    print(\"\\nClassification report (VALIDATION set):\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_val_true,\n",
    "            y_val_pred,\n",
    "            target_names=[IDX_TO_AAMI_LABEL[i] for i in range(5)],\n",
    "            digits=3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        y_val_true,\n",
    "        y_val_pred,\n",
    "        filename=\"confusion_matrix_val_snntorch.png\",\n",
    "        title=\"Confusion Matrix (Validation, snnTorch)\",\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 7) Evaluate on TEST set\n",
    "    # --------------------------------------------------------\n",
    "    test_loss, test_acc = eval_one_epoch(model, test_loader, device)\n",
    "    print(f\"\\n[TEST] loss={test_loss:.4f}, acc={test_acc:.3f}\")\n",
    "\n",
    "    y_test_true, y_test_pred = get_all_predictions(model, test_loader, device)\n",
    "\n",
    "    print(\"\\nClassification report (TEST set):\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test_true,\n",
    "            y_test_pred,\n",
    "            target_names=[IDX_TO_AAMI_LABEL[i] for i in range(5)],\n",
    "            digits=3,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        y_test_true,\n",
    "        y_test_pred,\n",
    "        filename=\"confusion_matrix_test_snntorch.png\",\n",
    "        title=\"Confusion Matrix (Test, snnTorch)\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b8d91-72c4-4d2d-a260-9b0f78e12862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN (Py3.10, CUDA 11.6)",
   "language": "python",
   "name": "snn_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
