{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce507d4-f668-4826-8b5d-09eb2d243497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diices/.conda/envs/snn_paper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda\n",
      "Class N: total=300, train=210, val=45, test=45\n",
      "Class S: total=300, train=210, val=45, test=45\n",
      "Class V: total=300, train=210, val=45, test=45\n",
      "Class F: total=300, train=210, val=45, test=45\n",
      "Class Q: total=300, train=210, val=45, test=45\n",
      "\n",
      "Total samples: 1500\n",
      "Train: 1050, Val: 225, Test: 225\n",
      "\n",
      "Sample shape: T=64, C=30 -> input_dim=30\n",
      "Epoch 001: train_loss=1.9027, train_acc=0.160, val_loss=1.6094, val_acc=0.200\n",
      "Epoch 002: train_loss=1.5357, train_acc=0.200, val_loss=1.4644, val_acc=0.200\n",
      "Epoch 003: train_loss=1.1177, train_acc=0.368, val_loss=1.0256, val_acc=0.400\n",
      "Epoch 004: train_loss=0.7591, train_acc=0.608, val_loss=0.2058, val_acc=0.898\n",
      "Epoch 005: train_loss=0.0816, train_acc=0.973, val_loss=0.0352, val_acc=0.987\n",
      "Epoch 006: train_loss=0.0288, train_acc=0.993, val_loss=0.0175, val_acc=0.996\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training SNN untuk ECG dengan snnTorch (3-way split: Train / Val / Test)\n",
    "------------------------------------------------------------------------\n",
    "Encoder terbaru: sampler_centered_var\n",
    "\n",
    "Dataset structure:\n",
    "    spikes_sampler_centered_var/\n",
    "        N/*_sampler_centered_var.npz\n",
    "        S/*_sampler_centered_var.npz\n",
    "        V/*_sampler_centered_var.npz\n",
    "        F/*_sampler_centered_var.npz\n",
    "        Q/*_sampler_centered_var.npz\n",
    "\n",
    "Setiap file .npz minimal punya:\n",
    "    - spikes         : [N_anchor, SLOTS] (uint8 0/1)\n",
    "    - (optional) label, fs, t_rel, anchor_times, anchor_indices, ...\n",
    "\n",
    "Input ke SNN:\n",
    "    spikes_np shape [N_anchor, SLOTS]\n",
    "    -> spikes_tensor [T, C] = [SLOTS, N_anchor]\n",
    "\n",
    "Model:\n",
    "    Input C -> FC(30)+LIF -> FC(30)+LIF -> FC(5)+LIF\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Label mapping AAMI\n",
    "# ============================================================\n",
    "\n",
    "AAMI_LABEL_TO_IDX: Dict[str, int] = {\"N\": 0, \"S\": 1, \"V\": 2, \"F\": 3, \"Q\": 4}\n",
    "IDX_TO_AAMI_LABEL: Dict[int, str] = {v: k for k, v in AAMI_LABEL_TO_IDX.items()}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Dataset: membaca sampler spike encoding dari .npz\n",
    "# ============================================================\n",
    "\n",
    "class SamplerSpikeECGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset untuk membaca spike encoding \"sampler_centered_var\".\n",
    "\n",
    "    Struktur folder:\n",
    "      root_dir/\n",
    "          N/*.npz\n",
    "          S/*.npz\n",
    "          V/*.npz\n",
    "          F/*.npz\n",
    "          Q/*.npz\n",
    "\n",
    "    Setiap file .npz minimal punya:\n",
    "      - spikes: [N_anchor, SLOTS]  (uint8 0/1)\n",
    "\n",
    "    Output __getitem__:\n",
    "      - spikes: Tensor [T, C] = [SLOTS, N_anchor] (float32)\n",
    "      - label : int (0..4)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, classes: Optional[List[str]] = None):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        if classes is None:\n",
    "            classes = [\"N\", \"S\", \"V\", \"F\", \"Q\"]\n",
    "        self.classes = [c.upper() for c in classes]\n",
    "\n",
    "        self.samples: List[Tuple[str, str]] = []\n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            files = sorted(glob.glob(os.path.join(cls_dir, \"*.npz\")))\n",
    "            for fp in files:\n",
    "                self.samples.append((fp, cls))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"Tidak ada file .npz di {root_dir}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        fp, cls = self.samples[idx]\n",
    "        d = np.load(fp, allow_pickle=True)\n",
    "\n",
    "        if \"spikes\" not in d.files:\n",
    "            d.close()\n",
    "            raise KeyError(f\"{fp} tidak memiliki key 'spikes'.\")\n",
    "\n",
    "        spikes_np = d[\"spikes\"]  # [N_anchor, SLOTS]\n",
    "        d.close()\n",
    "\n",
    "        # pastikan binary float32 (0/1)\n",
    "        spikes_np = (spikes_np > 0).astype(np.float32)\n",
    "\n",
    "        # Convert ke [T, C] = [SLOTS, N_anchor]\n",
    "        # spikes_np: [C, T] -> transpose\n",
    "        spikes = torch.from_numpy(spikes_np.T)  # [T, C]\n",
    "\n",
    "        y = AAMI_LABEL_TO_IDX[cls]\n",
    "        return spikes, y\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Stratified 3-way split: Train / Val / Test\n",
    "# ============================================================\n",
    "\n",
    "def split_dataset_3way(\n",
    "    dataset: SamplerSpikeECGDataset,\n",
    "    val_ratio: float = 0.15,\n",
    "    test_ratio: float = 0.15,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    class_to_indices: Dict[str, List[int]] = {c: [] for c in AAMI_LABEL_TO_IDX.keys()}\n",
    "    for i in range(len(dataset)):\n",
    "        _, cls = dataset.samples[i]\n",
    "        class_to_indices[cls].append(i)\n",
    "\n",
    "    train_idx: List[int] = []\n",
    "    val_idx: List[int] = []\n",
    "    test_idx: List[int] = []\n",
    "\n",
    "    for cls, indices in class_to_indices.items():\n",
    "        if not indices:\n",
    "            continue\n",
    "        rng.shuffle(indices)\n",
    "        n = len(indices)\n",
    "        n_val = int(n * val_ratio)\n",
    "        n_test = int(n * test_ratio)\n",
    "\n",
    "        cls_val = indices[:n_val]\n",
    "        cls_test = indices[n_val:n_val + n_test]\n",
    "        cls_train = indices[n_val + n_test:]\n",
    "\n",
    "        train_idx.extend(cls_train)\n",
    "        val_idx.extend(cls_val)\n",
    "        test_idx.extend(cls_test)\n",
    "\n",
    "        print(f\"Class {cls}: total={n}, train={len(cls_train)}, val={len(cls_val)}, test={len(cls_test)}\")\n",
    "\n",
    "    return (\n",
    "        torch.utils.data.Subset(dataset, train_idx),\n",
    "        torch.utils.data.Subset(dataset, val_idx),\n",
    "        torch.utils.data.Subset(dataset, test_idx),\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. SNN model (snnTorch)\n",
    "# ============================================================\n",
    "\n",
    "class SNN_Snntorch_ECG(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden1: int = 30, hidden2: int = 30, n_classes: int = 5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1, bias=True)\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2, bias=True)\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden2, n_classes, bias=True)\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=0.9, threshold=1.0,\n",
    "            spike_grad=surrogate.atan(),\n",
    "            learn_beta=True, learn_threshold=True,\n",
    "            reset_mechanism=\"subtract\",\n",
    "        )\n",
    "\n",
    "    def forward(self, spikes: torch.Tensor):\n",
    "        \"\"\"\n",
    "        spikes: [B, T, C]\n",
    "        return: [B, n_classes] spike counts\n",
    "        \"\"\"\n",
    "        B, T, C = spikes.shape\n",
    "        device = spikes.device\n",
    "\n",
    "        spikes_t = spikes.permute(1, 0, 2)  # [T, B, C]\n",
    "\n",
    "        mem1 = torch.zeros(B, self.fc1.out_features, device=device)\n",
    "        mem2 = torch.zeros(B, self.fc2.out_features, device=device)\n",
    "        mem3 = torch.zeros(B, self.fc3.out_features, device=device)\n",
    "\n",
    "        spk_sum = torch.zeros(B, self.fc3.out_features, device=device)\n",
    "\n",
    "        for t in range(T):\n",
    "            cur = spikes_t[t]          # [B, C]\n",
    "\n",
    "            cur = self.fc1(cur)\n",
    "            spk1, mem1 = self.lif1(cur, mem1)\n",
    "\n",
    "            cur = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur, mem2)\n",
    "\n",
    "            cur = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur, mem3)\n",
    "\n",
    "            spk_sum += spk3\n",
    "\n",
    "        return spk_sum\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Train / Eval helpers\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for spikes, labels in loader:\n",
    "        spikes = spikes.to(device)  # [B, T, C]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(spikes)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for spikes, labels in loader:\n",
    "        spikes = spikes.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(spikes)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_correct += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    ys = []\n",
    "    ps = []\n",
    "    for spikes, labels in loader:\n",
    "        spikes = spikes.to(device)\n",
    "        logits = model(spikes)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        ys.append(labels.numpy())\n",
    "        ps.append(preds)\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, filename, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    im = plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "    tick_marks = np.arange(5)\n",
    "    class_names = [IDX_TO_AAMI_LABEL[i] for i in range(5)]\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]),\n",
    "                     ha=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved as '{filename}'\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Main\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    dataset_dir = \"spikes_sampler_centered_var\"  # <-- GANTI ke folder encoder terbaru kamu\n",
    "    batch_size = 32\n",
    "    max_epochs = 100\n",
    "    lr = 1e-3\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    patience = 10\n",
    "    num_workers = 0\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device = {device}\")\n",
    "\n",
    "    full_dataset = SamplerSpikeECGDataset(dataset_dir, classes=[\"N\", \"S\", \"V\", \"F\", \"Q\"])\n",
    "    train_set, val_set, test_set = split_dataset_3way(full_dataset, val_ratio=val_ratio, test_ratio=test_ratio, seed=42)\n",
    "\n",
    "    print(f\"\\nTotal samples: {len(full_dataset)}\")\n",
    "    print(f\"Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\\n\")\n",
    "\n",
    "    # infer dim: sample [T, C]\n",
    "    spikes0, _ = full_dataset[0]\n",
    "    T0, C0 = spikes0.shape\n",
    "    input_dim = C0\n",
    "    print(f\"Sample shape: T={T0}, C={C0} -> input_dim={input_dim}\")\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "    val_loader   = DataLoader(val_set,   batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "    model = SNN_Snntorch_ECG(input_dim=input_dim, hidden1=30, hidden2=30, n_classes=5).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    train_acc_hist,  val_acc_hist  = [], []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        va_loss, va_acc = eval_one_epoch(model, val_loader, device)\n",
    "\n",
    "        train_loss_hist.append(tr_loss)\n",
    "        val_loss_hist.append(va_loss)\n",
    "        train_acc_hist.append(tr_acc)\n",
    "        val_acc_hist.append(va_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}: train_loss={tr_loss:.4f}, train_acc={tr_acc:.3f}, val_loss={va_loss:.4f}, val_acc={va_acc:.3f}\")\n",
    "\n",
    "        if va_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = va_loss\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        torch.save(best_state, \"snn_ecg_snntorch_sampler_30_30_best.pt\")\n",
    "        print(\"Best model saved to 'snn_ecg_snntorch_sampler_30_30_best.pt'\")\n",
    "\n",
    "    # curves\n",
    "    epochs_range = range(1, len(train_loss_hist) + 1)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(epochs_range, train_loss_hist, label=\"Train loss\")\n",
    "    plt.plot(epochs_range, val_loss_hist, label=\"Val loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training & Validation Loss (Sampler Encoder)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(epochs_range, train_acc_hist, label=\"Train acc\")\n",
    "    plt.plot(epochs_range, val_acc_hist, label=\"Val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training & Validation Accuracy (Sampler Encoder)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves_snntorch_sampler.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Training curves saved as 'training_curves_snntorch_sampler.png'\")\n",
    "\n",
    "    # validation report + CM\n",
    "    y_val_true, y_val_pred = get_all_predictions(model, val_loader, device)\n",
    "    print(\"\\nClassification report (VALIDATION set):\")\n",
    "    print(classification_report(\n",
    "        y_val_true, y_val_pred,\n",
    "        target_names=[IDX_TO_AAMI_LABEL[i] for i in range(5)],\n",
    "        digits=3,\n",
    "    ))\n",
    "    plot_confusion_matrix(y_val_true, y_val_pred,\n",
    "                          filename=\"confusion_matrix_val_snntorch_sampler.png\",\n",
    "                          title=\"Confusion Matrix (Validation, Sampler)\")\n",
    "\n",
    "    # test report + CM\n",
    "    te_loss, te_acc = eval_one_epoch(model, test_loader, device)\n",
    "    print(f\"\\n[TEST] loss={te_loss:.4f}, acc={te_acc:.3f}\")\n",
    "\n",
    "    y_test_true, y_test_pred = get_all_predictions(model, test_loader, device)\n",
    "    print(\"\\nClassification report (TEST set):\")\n",
    "    print(classification_report(\n",
    "        y_test_true, y_test_pred,\n",
    "        target_names=[IDX_TO_AAMI_LABEL[i] for i in range(5)],\n",
    "        digits=3,\n",
    "    ))\n",
    "    plot_confusion_matrix(y_test_true, y_test_pred,\n",
    "                          filename=\"confusion_matrix_test_snntorch_sampler.png\",\n",
    "                          title=\"Confusion Matrix (Test, Sampler)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b8d91-72c4-4d2d-a260-9b0f78e12862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN (Py3.10, CUDA 11.6)",
   "language": "python",
   "name": "snn_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
